{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Clone Detection Dataset Generation Pipeline\n",
    "\n",
    "## Overview\n",
    "This notebook generates a complete code clone detection dataset with 1M+ examples.\n",
    "\n",
    "**Dataset Composition:**\n",
    "- **Type-1 Clones** (10%): Exact copies with whitespace/comment variations\n",
    "- **Type-2 Clones** (20%): Copies with renamed identifiers\n",
    "- **Type-3 Clones** (10%): Copies with structural modifications\n",
    "- **Type-4 Clones** (10%): Semantically similar, syntactically different\n",
    "- **Non-clones (Easy)** (25%): Different problems\n",
    "- **Non-clones (Hard)** (25%): Same problem, different approaches\n",
    "\n",
    "**Supported Languages:** Java, C, C++, Go, Python, JavaScript, C#\n",
    "\n",
    "**Tools:** Tree-sitter for parsing, PolyglotParser for multi-language support"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 1: Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-07T16:08:55.552873Z",
     "start_time": "2025-12-07T16:08:54.888875Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 21:58:01 - INFO - Output directory ready: /home/dasunwickr/SLIIT/Y4S1/4YRG/Datasets/processed_clones\n",
      "2025-12-07 21:58:01 - INFO - PolyglotParser initialized successfully\n",
      "2025-12-07 21:58:01 - INFO - PolyglotParser initialized successfully\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CODE CLONE DATASET GENERATION PIPELINE\n",
      "================================================================================\n",
      "\n",
      "Dataset Root: /home/dasunwickr/SLIIT/Y4S1/4YRG/Datasets/Project_CodeNet\n",
      "Output Directory: /home/dasunwickr/SLIIT/Y4S1/4YRG/Datasets/processed_clones\n",
      "Target Pairs: 100,000\n",
      "Languages: java, python, c, cpp, go, javascript, c_sharp\n",
      "Minimum Code Size: 50 bytes\n",
      "\n",
      "Clone Type Distribution:\n",
      "   type1: 100% (100,000 pairs)\n",
      "   type2: 100% (100,000 pairs)\n",
      "   type3: 100% (100,000 pairs)\n",
      "   type4: 100% (100,000 pairs)\n",
      "   negative_easy: 100% (100,000 pairs)\n",
      "   negative_hard: 100% (100,000 pairs)\n",
      "\n",
      "Dataset Splits:\n",
      "   train: 60% (60,000 pairs)\n",
      "   val: 20% (20,000 pairs)\n",
      "   test: 20% (20,000 pairs)\n",
      "================================================================================\n",
      "\n",
      "Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Import helper modules from scripts folder\n",
    "from scripts.config import (\n",
    "    CODENET_ROOT,\n",
    "    OUTPUT_DIR,\n",
    "    LANGUAGES,\n",
    "    TARGET_PAIRS,\n",
    "    CLONE_TYPE_RATIOS,\n",
    "    SPLIT_RATIOS,\n",
    "    MIN_CODE_SIZE\n",
    ")\n",
    "from scripts.parser import PolyglotParser\n",
    "from scripts.indexer import build_master_index\n",
    "from scripts.clone_generators import (\n",
    "    generate_type1,\n",
    "    generate_type2,\n",
    "    generate_type3,\n",
    "    get_type4_pairs,\n",
    "    sample_clone_pairs\n",
    ")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Override output directory to save in processed folder\n",
    "OUTPUT_DIR_OVERRIDE = Path(__file__).parent / \"processed\" if '__file__' in globals() else Path.cwd() / \"processed\"\n",
    "\n",
    "# Configuration\n",
    "print(\"=\"*80)\n",
    "print(\"CODE CLONE DATASET GENERATION PIPELINE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset Root: {CODENET_ROOT}\")\n",
    "print(f\"Output Directory: {OUTPUT_DIR_OVERRIDE}\")\n",
    "print(f\"Target Pairs: {TARGET_PAIRS:,}\")\n",
    "print(f\"Languages: {', '.join(LANGUAGES)}\")\n",
    "print(f\"Minimum Code Size: {MIN_CODE_SIZE} bytes\")\n",
    "print(f\"\\nClone Type Distribution:\")\n",
    "for clone_type, ratio in CLONE_TYPE_RATIOS.items():\n",
    "    # Every submission will have all clone types, so report 100% and full TARGET_PAIRS count\n",
    "    count = TARGET_PAIRS\n",
    "    print(f\"   {clone_type}: 100% ({count:,} pairs)\")\n",
    "print(f\"\\nDataset Splits:\")\n",
    "for split, ratio in SPLIT_RATIOS.items():\n",
    "    count = int(TARGET_PAIRS * ratio)\n",
    "    print(f\"   {split}: {ratio*100:.0f}% ({count:,} pairs)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create output directory\n",
    "output_path = Path(OUTPUT_DIR_OVERRIDE)\n",
    "output_path.mkdir(parents=True, exist_ok=True)\n",
    "logger.info(f\"Output directory ready: {output_path}\")\n",
    "\n",
    "# Initialize parser\n",
    "parser = PolyglotParser()\n",
    "logger.info(\"PolyglotParser initialized successfully\")\n",
    "\n",
    "print(\"\\nSetup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 2: Load Program List and Build Master Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 21:58:21 - INFO - Building master index from CodeNet metadata...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "BUILDING MASTER INDEX\n",
      "================================================================================\n",
      "Loading cached master index from /home/dasunwickr/SLIIT/Y4S1/4YRG/Datasets/Project_CodeNet/processed/master_index.pkl\n",
      "\n",
      "Index Statistics:\n",
      "   Total Problems: 3,593\n",
      "   Total Accepted Submissions: 2,451,134\n",
      "\n",
      "Language Distribution:\n",
      "   python: 1,699,085 submissions\n",
      "   java: 354,982 submissions\n",
      "   c: 313,129 submissions\n",
      "   go: 58,404 submissions\n",
      "   javascript: 25,534 submissions\n",
      "\n",
      "Potential Clone Pairs: 2,335,361,461\n",
      "   (This is enough to generate 100,000+ pairs)\n",
      "\n",
      "Master index built successfully!\n",
      "   Ready to generate 100,000 code clone pairs\n",
      "================================================================================\n",
      "\n",
      "Index Statistics:\n",
      "   Total Problems: 3,593\n",
      "   Total Accepted Submissions: 2,451,134\n",
      "\n",
      "Language Distribution:\n",
      "   python: 1,699,085 submissions\n",
      "   java: 354,982 submissions\n",
      "   c: 313,129 submissions\n",
      "   go: 58,404 submissions\n",
      "   javascript: 25,534 submissions\n",
      "\n",
      "Potential Clone Pairs: 2,335,361,461\n",
      "   (This is enough to generate 100,000+ pairs)\n",
      "\n",
      "Master index built successfully!\n",
      "   Ready to generate 100,000 code clone pairs\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BUILDING MASTER INDEX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Build master index of all accepted submissions\n",
    "logger.info(\"Building master index from CodeNet metadata...\")\n",
    "master_index = build_master_index(\n",
    "    codenet_root=CODENET_ROOT,\n",
    "    languages=LANGUAGES,\n",
    "    min_code_size=MIN_CODE_SIZE,\n",
    "    use_cache=True\n",
    ")\n",
    "\n",
    "# Analyze the index\n",
    "total_problems = len(master_index)\n",
    "total_submissions = sum(len(subs) for lang_dict in master_index.values() for subs in lang_dict.values())\n",
    "\n",
    "print(f\"\\nIndex Statistics:\")\n",
    "print(f\"   Total Problems: {total_problems:,}\")\n",
    "print(f\"   Total Accepted Submissions: {total_submissions:,}\")\n",
    "\n",
    "# Language distribution\n",
    "lang_counts = {}\n",
    "for problem_id, lang_dict in master_index.items():\n",
    "    for lang, subs in lang_dict.items():\n",
    "        if lang not in lang_counts:\n",
    "            lang_counts[lang] = 0\n",
    "        lang_counts[lang] += len(subs)\n",
    "\n",
    "print(f\"\\nLanguage Distribution:\")\n",
    "for lang, count in sorted(lang_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"   {lang}: {count:,} submissions\")\n",
    "\n",
    "# Estimate potential pairs\n",
    "potential_pairs = sum(\n",
    "    len(subs) * (len(subs) - 1) // 2\n",
    "    for lang_dict in master_index.values()\n",
    "    for subs in lang_dict.values()\n",
    "    if len(subs) >= 2\n",
    ")\n",
    "print(f\"\\nPotential Clone Pairs: {potential_pairs:,}\")\n",
    "print(f\"   (This is enough to generate {TARGET_PAIRS:,}+ pairs)\")\n",
    "\n",
    "# Sample problems for dataset generation\n",
    "all_problems = list(master_index.keys())\n",
    "random.shuffle(all_problems)\n",
    "\n",
    "print(f\"\\nMaster index built successfully!\")\n",
    "print(f\"   Ready to generate {TARGET_PAIRS:,} code clone pairs\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 3: Generate Type-1 Clones\n",
    "\n",
    "Type-1 clones are exact copies with variations in whitespace and comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 21:58:33 - INFO - Target Type-1 pairs: 1,000,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING TYPE-1 CLONES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type-1 Generation:   5%|▌         | 52842/1000000 [04:24<1:18:52, 200.12pairs/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated 52,842 Type-1 clone pairs\n",
      "   Target: 1,000,000\n",
      "   Success rate: 5.3%\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TYPE-1 CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_type1 = 10000  # 1M pairs for Type-1\n",
    "logger.info(f\"Target Type-1 pairs: {target_type1:,}\")\n",
    "\n",
    "type1_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_type1, desc=\"Type-1 Generation\", unit=\"pairs\")\n",
    "\n",
    "# Generate Type-1 clones by removing comments and normalizing whitespace\n",
    "for problem_id in all_problems:\n",
    "    if len(type1_data) >= target_type1:\n",
    "        break\n",
    "    \n",
    "    lang_dict = master_index[problem_id]\n",
    "    \n",
    "    for lang, submissions in lang_dict.items():\n",
    "        if len(type1_data) >= target_type1:\n",
    "            break\n",
    "        \n",
    "        if len(submissions) < 1:\n",
    "            continue\n",
    "        \n",
    "        # Load a submission and generate its Type-1 normalized version\n",
    "        from scripts.clone_generators import _load_code_from_submission\n",
    "        \n",
    "        for sub_id in submissions[:min(5, len(submissions))]:  # Limit per problem\n",
    "            if len(type1_data) >= target_type1:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                original_code = _load_code_from_submission(\n",
    "                    CODENET_ROOT, problem_id, sub_id, lang\n",
    "                )\n",
    "                \n",
    "                if len(original_code.strip()) < MIN_CODE_SIZE:\n",
    "                    continue\n",
    "                \n",
    "                # Generate Type-1 normalized version\n",
    "                type1_code = generate_type1(original_code, lang, parser)\n",
    "                \n",
    "                if type1_code and type1_code != original_code:\n",
    "                    type1_data.append({\n",
    "                        'id': f'type1_{pair_id}',\n",
    "                        'code_1': original_code,\n",
    "                        'code_2': type1_code,\n",
    "                        'label': 1,\n",
    "                        'type': 'type1',\n",
    "                        'language': lang,\n",
    "                        'problem_id': problem_id\n",
    "                    })\n",
    "                    pair_id += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error generating Type-1 for {problem_id}/{sub_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "type1_df = pd.DataFrame(type1_data)\n",
    "print(f\"\\nGenerated {len(type1_df):,} Type-1 clone pairs\")\n",
    "print(f\"   Target: {target_type1:,}\")\n",
    "print(f\"   Success rate: {len(type1_df)/target_type1*100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 4: Generate Type-2 Clones\n",
    "\n",
    "Type-2 clones are copies with renamed identifiers (variables, functions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 22:04:09 - INFO - Target Type-2 pairs: 1,000,000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "GENERATING TYPE-2 CLONES\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Type-2 Generation:   5%|▌         | 50820/1000000 [05:26<3:54:08, 67.57pairs/s] "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     40\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Generate Type-2 with renamed identifiers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m type2_code = \u001b[43mgenerate_type2\u001b[49m\u001b[43m(\u001b[49m\u001b[43moriginal_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m type2_code \u001b[38;5;129;01mand\u001b[39;00m type2_code != original_code:\n\u001b[32m     46\u001b[39m     type2_data.append({\n\u001b[32m     47\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m: \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mtype2_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpair_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m,\n\u001b[32m     48\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mcode_1\u001b[39m\u001b[33m'\u001b[39m: original_code,\n\u001b[32m   (...)\u001b[39m\u001b[32m     53\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mproblem_id\u001b[39m\u001b[33m'\u001b[39m: problem_id\n\u001b[32m     54\u001b[39m     })\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:421\u001b[39m, in \u001b[36mgenerate_type2\u001b[39m\u001b[34m(code, lang, parser)\u001b[39m\n\u001b[32m    418\u001b[39m code_bytes = code.encode(\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    420\u001b[39m \u001b[38;5;66;03m# Collect all renameable identifiers\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m421\u001b[39m renameable_identifiers = \u001b[43m_collect_renameable_identifiers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    423\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m renameable_identifiers:\n\u001b[32m    424\u001b[39m     \u001b[38;5;66;03m# No identifiers to rename, return original code\u001b[39;00m\n\u001b[32m    425\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m code\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:362\u001b[39m, in \u001b[36m_collect_renameable_identifiers\u001b[39m\u001b[34m(tree, lang)\u001b[39m\n\u001b[32m    359\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node.children:\n\u001b[32m    360\u001b[39m         traverse(child)\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m \u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mroot_node\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    363\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m renameable_identifiers\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:360\u001b[39m, in \u001b[36m_collect_renameable_identifiers.<locals>.traverse\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Continue traversing children\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node.children:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:360\u001b[39m, in \u001b[36m_collect_renameable_identifiers.<locals>.traverse\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Continue traversing children\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node.children:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[31m[... skipping similar frames: _collect_renameable_identifiers.<locals>.traverse at line 360 (5 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:360\u001b[39m, in \u001b[36m_collect_renameable_identifiers.<locals>.traverse\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;66;03m# Continue traversing children\u001b[39;00m\n\u001b[32m    359\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m node.children:\n\u001b[32m--> \u001b[39m\u001b[32m360\u001b[39m     \u001b[43mtraverse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchild\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:345\u001b[39m, in \u001b[36m_collect_renameable_identifiers.<locals>.traverse\u001b[39m\u001b[34m(node)\u001b[39m\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# Skip if it's in an import/module context\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m345\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_is_import_or_module_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexcluded_types\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    346\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    348\u001b[39m \u001b[38;5;66;03m# Skip if it starts with underscore (often special/magic methods)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/4YRG/gradeloop-core/apps/cipas-service/app/code-clone-detection/dataset-prep/scripts/clone_generators.py:309\u001b[39m, in \u001b[36m_is_import_or_module_context\u001b[39m\u001b[34m(node, excluded_types)\u001b[39m\n\u001b[32m    307\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m current.type \u001b[38;5;129;01min\u001b[39;00m excluded_types:\n\u001b[32m    308\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m309\u001b[39m     current = current.parent\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TYPE-2 CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_type2 = 10000  # 1M pairs for Type-\n",
    "logger.info(f\"Target Type-2 pairs: {target_type2:,}\")\n",
    "\n",
    "type2_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_type2, desc=\"Type-2 Generation\", unit=\"pairs\")\n",
    "\n",
    "# Generate Type-2 clones by renaming identifiers\n",
    "for problem_id in all_problems:\n",
    "    if len(type2_data) >= target_type2:\n",
    "        break\n",
    "    \n",
    "    lang_dict = master_index[problem_id]\n",
    "    \n",
    "    for lang, submissions in lang_dict.items():\n",
    "        if len(type2_data) >= target_type2:\n",
    "            break\n",
    "        \n",
    "        if len(submissions) < 1:\n",
    "            continue\n",
    "        \n",
    "        from scripts.clone_generators import _load_code_from_submission\n",
    "        \n",
    "        for sub_id in submissions[:min(10, len(submissions))]:  # More per problem for Type-2\n",
    "            if len(type2_data) >= target_type2:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                original_code = _load_code_from_submission(\n",
    "                    CODENET_ROOT, problem_id, sub_id, lang\n",
    "                )\n",
    "                \n",
    "                if len(original_code.strip()) < MIN_CODE_SIZE:\n",
    "                    continue\n",
    "                \n",
    "                # Generate Type-2 with renamed identifiers\n",
    "                type2_code = generate_type2(original_code, lang, parser)\n",
    "                \n",
    "                if type2_code and type2_code != original_code:\n",
    "                    type2_data.append({\n",
    "                        'id': f'type2_{pair_id}',\n",
    "                        'code_1': original_code,\n",
    "                        'code_2': type2_code,\n",
    "                        'label': 1,\n",
    "                        'type': 'type2',\n",
    "                        'language': lang,\n",
    "                        'problem_id': problem_id\n",
    "                    })\n",
    "                    pair_id += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error generating Type-2 for {problem_id}/{sub_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "type2_df = pd.DataFrame(type2_data)\n",
    "print(f\"\\nGenerated {len(type2_df):,} Type-2 clone pairs\")\n",
    "print(f\"   Target: {target_type2:,}\")\n",
    "print(f\"   Success rate: {len(type2_df)/target_type2*100:.1f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 5: Generate Type-3 Clones\n",
    "\n",
    "Type-3 clones are copies with structural modifications (loop transformations, added statements)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TYPE-3 CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_type3 = 10000  # 1M pairs for Type-3\n",
    "logger.info(f\"Target Type-3 pairs: {target_type3:,}\")\n",
    "\n",
    "type3_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Check if LLM is available (optional)\n",
    "USE_LLM = True  # Set to True if Ollama is running locally\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_type3, desc=\"Type-3 Generation\", unit=\"pairs\")\n",
    "\n",
    "# Generate Type-3 clones with structural mutations\n",
    "for problem_id in all_problems:\n",
    "    if len(type3_data) >= target_type3:\n",
    "        break\n",
    "    \n",
    "    lang_dict = master_index[problem_id]\n",
    "    \n",
    "    for lang, submissions in lang_dict.items():\n",
    "        if len(type3_data) >= target_type3:\n",
    "            break\n",
    "        \n",
    "        if len(submissions) < 1:\n",
    "            continue\n",
    "        \n",
    "        from scripts.clone_generators import _load_code_from_submission\n",
    "        \n",
    "        for sub_id in submissions[:min(5, len(submissions))]:\n",
    "            if len(type3_data) >= target_type3:\n",
    "                break\n",
    "            \n",
    "            try:\n",
    "                original_code = _load_code_from_submission(\n",
    "                    CODENET_ROOT, problem_id, sub_id, lang\n",
    "                )\n",
    "                \n",
    "                if len(original_code.strip()) < MIN_CODE_SIZE:\n",
    "                    continue\n",
    "                \n",
    "                # Generate Type-3 with structural mutations\n",
    "                type3_code = generate_type3(\n",
    "                    original_code, lang, use_llm=USE_LLM, parser=parser\n",
    "                )\n",
    "                \n",
    "                if type3_code and type3_code != original_code:\n",
    "                    type3_data.append({\n",
    "                        'id': f'type3_{pair_id}',\n",
    "                        'code_1': original_code,\n",
    "                        'code_2': type3_code,\n",
    "                        'label': 1,\n",
    "                        'type': 'type3',\n",
    "                        'language': lang,\n",
    "                        'problem_id': problem_id\n",
    "                    })\n",
    "                    pair_id += 1\n",
    "                    pbar.update(1)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.debug(f\"Error generating Type-3 for {problem_id}/{sub_id}: {e}\")\n",
    "                continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "type3_df = pd.DataFrame(type3_data)\n",
    "print(f\"\\nGenerated {len(type3_df):,} Type-3 clone pairs\")\n",
    "print(f\"   Target: {target_type3:,}\")\n",
    "print(f\"   Success rate: {len(type3_df)/target_type3*100:.1f}%\")\n",
    "print(f\"   Method: {'LLM-based' if USE_LLM else 'Rule-based'}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 6: Generate Type-4 Clones\n",
    "\n",
    "Type-4 clones are semantically similar but syntactically different implementations of the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING TYPE-4 CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_type4 = 10000  # 1M pairs for Type-4\n",
    "logger.info(f\"Target Type-4 pairs: {target_type4:,}\")\n",
    "\n",
    "type4_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Type-4 threshold for Jaccard similarity\n",
    "TYPE4_THRESHOLD = 0.4\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_type4, desc=\"Type-4 Generation\", unit=\"pairs\")\n",
    "\n",
    "# Generate Type-4 clones by finding low-similarity pairs from same problem\n",
    "for problem_id in all_problems:\n",
    "    if len(type4_data) >= target_type4:\n",
    "        break\n",
    "    \n",
    "    lang_dict = master_index[problem_id]\n",
    "    \n",
    "    for lang, submissions in lang_dict.items():\n",
    "        if len(type4_data) >= target_type4:\n",
    "            break\n",
    "        \n",
    "        if len(submissions) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Build submissions dictionary for get_type4_pairs\n",
    "        from scripts.clone_generators import _load_code_from_submission\n",
    "        \n",
    "        submissions_dict = {}\n",
    "        for sub_id in submissions[:min(20, len(submissions))]:\n",
    "            try:\n",
    "                code = _load_code_from_submission(CODENET_ROOT, problem_id, sub_id, lang)\n",
    "                if len(code.strip()) >= MIN_CODE_SIZE:\n",
    "                    submissions_dict[sub_id] = {\n",
    "                        'code': code,\n",
    "                        'language': lang,\n",
    "                        'status': 'accepted'\n",
    "                    }\n",
    "            except Exception:\n",
    "                continue\n",
    "        \n",
    "        if len(submissions_dict) < 2:\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            # Get Type-4 pairs using Jaccard similarity\n",
    "            type4_pairs = get_type4_pairs(\n",
    "                problem_id, submissions_dict, parser, threshold=TYPE4_THRESHOLD\n",
    "            )\n",
    "            \n",
    "            for code1, code2, lang1, lang2 in type4_pairs:\n",
    "                if len(type4_data) >= target_type4:\n",
    "                    break\n",
    "                \n",
    "                type4_data.append({\n",
    "                    'id': f'type4_{pair_id}',\n",
    "                    'code_1': code1,\n",
    "                    'code_2': code2,\n",
    "                    'label': 1,\n",
    "                    'type': 'type4',\n",
    "                    'language': lang1,  # Could be different languages\n",
    "                    'problem_id': problem_id\n",
    "                })\n",
    "                pair_id += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"Error generating Type-4 for {problem_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "type4_df = pd.DataFrame(type4_data)\n",
    "print(f\"\\nGenerated {len(type4_df):,} Type-4 clone pairs\")\n",
    "print(f\"   Target: {target_type4:,}\")\n",
    "print(f\"   Success rate: {len(type4_df)/target_type4*100:.1f}%\")\n",
    "print(f\"   Similarity threshold: < {TYPE4_THRESHOLD}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 7: Generate Non-clones (Easy)\n",
    "\n",
    "Easy non-clones are pairs from completely different problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING EASY NON-CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_negative_easy = 1_000_000  # 1M pairs for Easy Non-clones\n",
    "logger.info(f\"Target Easy Non-clones: {target_negative_easy:,}\")\n",
    "\n",
    "negative_easy_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_negative_easy, desc=\"Easy Non-clone Generation\", unit=\"pairs\")\n",
    "\n",
    "# Track used problems to avoid duplication\n",
    "used_problem_pairs = set()\n",
    "\n",
    "# Generate easy non-clones from different problems\n",
    "attempts = 0\n",
    "max_attempts = target_negative_easy * 20\n",
    "\n",
    "from scripts.clone_generators import _load_code_from_submission\n",
    "\n",
    "while len(negative_easy_data) < target_negative_easy and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    \n",
    "    # Select two random different problems\n",
    "    if len(all_problems) < 2:\n",
    "        break\n",
    "    \n",
    "    problem1, problem2 = random.sample(all_problems, 2)\n",
    "    \n",
    "    # Skip if we've already used this pair\n",
    "    pair_key = tuple(sorted([problem1, problem2]))\n",
    "    if pair_key in used_problem_pairs:\n",
    "        continue\n",
    "    \n",
    "    used_problem_pairs.add(pair_key)\n",
    "    \n",
    "    # Find common languages\n",
    "    langs1 = set(master_index[problem1].keys())\n",
    "    langs2 = set(master_index[problem2].keys())\n",
    "    common_langs = langs1.intersection(langs2)\n",
    "    \n",
    "    if not common_langs:\n",
    "        continue\n",
    "    \n",
    "    lang = random.choice(list(common_langs))\n",
    "    \n",
    "    # Get submissions\n",
    "    subs1 = master_index[problem1][lang]\n",
    "    subs2 = master_index[problem2][lang]\n",
    "    \n",
    "    if not subs1 or not subs2:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        sub1 = random.choice(subs1)\n",
    "        sub2 = random.choice(subs2)\n",
    "        \n",
    "        code1 = _load_code_from_submission(CODENET_ROOT, problem1, sub1, lang)\n",
    "        code2 = _load_code_from_submission(CODENET_ROOT, problem2, sub2, lang)\n",
    "        \n",
    "        if len(code1.strip()) < MIN_CODE_SIZE or len(code2.strip()) < MIN_CODE_SIZE:\n",
    "            continue\n",
    "        \n",
    "        negative_easy_data.append({\n",
    "            'id': f'neg_easy_{pair_id}',\n",
    "            'code_1': code1,\n",
    "            'code_2': code2,\n",
    "            'label': 0,\n",
    "            'type': 'negative_easy',\n",
    "            'language': lang,\n",
    "            'problem_id': f'{problem1}|{problem2}'\n",
    "        })\n",
    "        pair_id += 1\n",
    "        pbar.update(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error generating easy non-clone: {e}\")\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "negative_easy_df = pd.DataFrame(negative_easy_data)\n",
    "print(f\"\\nGenerated {len(negative_easy_df):,} Easy Non-clone pairs\")\n",
    "print(f\"   Target: {target_negative_easy:,}\")\n",
    "print(f\"   Success rate: {len(negative_easy_df)/target_negative_easy*100:.1f}%\")\n",
    "print(f\"   Attempts: {attempts:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 8: Generate Non-clones (Hard)\n",
    "\n",
    "Hard non-clones are pairs from the same problem but with low similarity (different approaches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"GENERATING HARD NON-CLONES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "target_negative_hard = 1_000_000  # 1M pairs for Hard Non-clones\n",
    "logger.info(f\"Target Hard Non-clones: {target_negative_hard:,}\")\n",
    "\n",
    "negative_hard_data = []\n",
    "pair_id = 0\n",
    "\n",
    "# Similarity threshold for hard negatives\n",
    "HARD_NEGATIVE_THRESHOLD = 0.6\n",
    "\n",
    "# Progress bar\n",
    "pbar = tqdm(total=target_negative_hard, desc=\"Hard Non-clone Generation\", unit=\"pairs\")\n",
    "\n",
    "# Generate hard non-clones from same problem with low similarity\n",
    "attempts = 0\n",
    "max_attempts = target_negative_hard * 20\n",
    "\n",
    "from scripts.clone_generators import _load_code_from_submission, _tokenize_code, _jaccard_similarity\n",
    "\n",
    "while len(negative_hard_data) < target_negative_hard and attempts < max_attempts:\n",
    "    attempts += 1\n",
    "    \n",
    "    if not all_problems:\n",
    "        break\n",
    "    \n",
    "    problem = random.choice(all_problems)\n",
    "    lang_dict = master_index[problem]\n",
    "    \n",
    "    if not lang_dict:\n",
    "        continue\n",
    "    \n",
    "    lang = random.choice(list(lang_dict.keys()))\n",
    "    submissions = lang_dict[lang]\n",
    "    \n",
    "    if len(submissions) < 2:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        sub1, sub2 = random.sample(submissions, 2)\n",
    "        \n",
    "        code1 = _load_code_from_submission(CODENET_ROOT, problem, sub1, lang)\n",
    "        code2 = _load_code_from_submission(CODENET_ROOT, problem, sub2, lang)\n",
    "        \n",
    "        if len(code1.strip()) < MIN_CODE_SIZE or len(code2.strip()) < MIN_CODE_SIZE:\n",
    "            continue\n",
    "        \n",
    "        # Check if similarity is low enough\n",
    "        try:\n",
    "            tokens1 = _tokenize_code(code1, lang, parser)\n",
    "            tokens2 = _tokenize_code(code2, lang, parser)\n",
    "            similarity = _jaccard_similarity(tokens1, tokens2)\n",
    "            \n",
    "            # Consider it a hard negative if similarity is low\n",
    "            if similarity < HARD_NEGATIVE_THRESHOLD:\n",
    "                negative_hard_data.append({\n",
    "                    'id': f'neg_hard_{pair_id}',\n",
    "                    'code_1': code1,\n",
    "                    'code_2': code2,\n",
    "                    'label': 0,\n",
    "                    'type': 'negative_hard',\n",
    "                    'language': lang,\n",
    "                    'problem_id': problem\n",
    "                })\n",
    "                pair_id += 1\n",
    "                pbar.update(1)\n",
    "        except Exception:\n",
    "            # On tokenization failure, treat as hard negative if codes are different\n",
    "            if code1 != code2:\n",
    "                negative_hard_data.append({\n",
    "                    'id': f'neg_hard_{pair_id}',\n",
    "                    'code_1': code1,\n",
    "                    'code_2': code2,\n",
    "                    'label': 0,\n",
    "                    'type': 'negative_hard',\n",
    "                    'language': lang,\n",
    "                    'problem_id': problem\n",
    "                })\n",
    "                pair_id += 1\n",
    "                pbar.update(1)\n",
    "                \n",
    "    except Exception as e:\n",
    "        logger.debug(f\"Error generating hard non-clone: {e}\")\n",
    "        continue\n",
    "\n",
    "pbar.close()\n",
    "\n",
    "negative_hard_df = pd.DataFrame(negative_hard_data)\n",
    "print(f\"\\nGenerated {len(negative_hard_df):,} Hard Non-clone pairs\")\n",
    "print(f\"   Target: {target_negative_hard:,}\")\n",
    "print(f\"   Success rate: {len(negative_hard_df)/target_negative_hard*100:.1f}%\")\n",
    "print(f\"   Similarity threshold: < {HARD_NEGATIVE_THRESHOLD}\")\n",
    "print(f\"   Attempts: {attempts:,}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 9: Combine, Shuffle, and Deduplicate\n",
    "\n",
    "Merge all clone types, remove duplicates, shuffle, and balance the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINING AND PROCESSING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine all dataframes\n",
    "all_dfs = [\n",
    "    type1_df,\n",
    "    type2_df,\n",
    "    type3_df,\n",
    "    type4_df,\n",
    "    negative_easy_df,\n",
    "    negative_hard_df\n",
    "]\n",
    "\n",
    "# Filter out empty dataframes\n",
    "all_dfs = [df for df in all_dfs if len(df) > 0]\n",
    "\n",
    "if not all_dfs:\n",
    "    raise ValueError(\"No data was generated! Check the configuration and source data.\")\n",
    "\n",
    "# Concatenate all data\n",
    "print(\"\\nCombining datasets...\")\n",
    "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "print(f\"   Total pairs before processing: {len(combined_df):,}\")\n",
    "\n",
    "# Print distribution before deduplication\n",
    "print(\"\\nDistribution by type (before deduplication):\")\n",
    "type_counts = combined_df['type'].value_counts()\n",
    "for clone_type, count in type_counts.items():\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   {clone_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Remove duplicates based on code content\n",
    "print(\"\\nRemoving duplicates...\")\n",
    "initial_size = len(combined_df)\n",
    "\n",
    "# Create a hash of code pairs for deduplication\n",
    "combined_df['pair_hash'] = combined_df.apply(\n",
    "    lambda row: hash(tuple(sorted([row['code_1'], row['code_2']]))),\n",
    "    axis=1\n",
    ")\n",
    "combined_df = combined_df.drop_duplicates(subset=['pair_hash'], keep='first')\n",
    "combined_df = combined_df.drop('pair_hash', axis=1)\n",
    "\n",
    "duplicates_removed = initial_size - len(combined_df)\n",
    "print(f\"   Removed {duplicates_removed:,} duplicate pairs\")\n",
    "print(f\"   Remaining pairs: {len(combined_df):,}\")\n",
    "\n",
    "# Shuffle the dataset\n",
    "print(\"\\nShuffling dataset...\")\n",
    "combined_df = combined_df.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
    "print(\"   Dataset shuffled successfully\")\n",
    "\n",
    "# Check label balance\n",
    "print(\"\\nLabel distribution:\")\n",
    "label_counts = combined_df['label'].value_counts()\n",
    "for label, count in label_counts.items():\n",
    "    label_name = \"Clone\" if label == 1 else \"Non-clone\"\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   {label_name} (label={label}): {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Final distribution by type\n",
    "print(\"\\nFinal distribution by type:\")\n",
    "type_counts = combined_df['type'].value_counts()\n",
    "for clone_type, count in type_counts.items():\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   {clone_type}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Language distribution\n",
    "print(\"\\nLanguage distribution:\")\n",
    "lang_counts = combined_df['language'].value_counts()\n",
    "for lang, count in lang_counts.items():\n",
    "    percentage = (count / len(combined_df)) * 100\n",
    "    print(f\"   {lang}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nDataset processing complete!\")\n",
    "print(f\"   Final size: {len(combined_df):,} pairs\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 10: Export Dataset\n",
    "\n",
    "Export the final dataset to CSV format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPORTING DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare output filename with timestamp\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_filename = f\"cipas_code_clone_dataset_{timestamp}.csv\"\n",
    "output_filepath = output_path / output_filename\n",
    "\n",
    "print(f\"\\nExporting to CSV...\")\n",
    "print(f\"   File: {output_filepath}\")\n",
    "\n",
    "# Export to CSV\n",
    "combined_df.to_csv(output_filepath, index=False, encoding='utf-8')\n",
    "\n",
    "# Verify the export\n",
    "file_size_mb = output_filepath.stat().st_size / (1024 * 1024)\n",
    "print(f\"   File size: {file_size_mb:.2f} MB\")\n",
    "print(f\"   Rows exported: {len(combined_df):,}\")\n",
    "print(f\"   Columns: {', '.join(combined_df.columns)}\")\n",
    "\n",
    "# Also save a version without timestamp for easy access\n",
    "main_output_file = output_path / \"cipas_code_clone_dataset.csv\"\n",
    "combined_df.to_csv(main_output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\n   Also saved as: {main_output_file}\")\n",
    "\n",
    "# Generate dataset statistics file\n",
    "stats_file = output_path / f\"dataset_stats_{timestamp}.txt\"\n",
    "with open(stats_file, 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"CIPAS CODE CLONE DATASET STATISTICS\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n",
    "    f.write(f\"Total Pairs: {len(combined_df):,}\\n\\n\")\n",
    "    \n",
    "    f.write(\"Label Distribution:\\n\")\n",
    "    for label, count in combined_df['label'].value_counts().items():\n",
    "        label_name = \"Clone\" if label == 1 else \"Non-clone\"\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        f.write(f\"  {label_name}: {count:,} ({percentage:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nType Distribution:\\n\")\n",
    "    for clone_type, count in combined_df['type'].value_counts().items():\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        f.write(f\"  {clone_type}: {count:,} ({percentage:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\nLanguage Distribution:\\n\")\n",
    "    for lang, count in combined_df['language'].value_counts().items():\n",
    "        percentage = (count / len(combined_df)) * 100\n",
    "        f.write(f\"  {lang}: {count:,} ({percentage:.1f}%)\\n\")\n",
    "    \n",
    "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "print(f\"   Statistics saved: {stats_file}\")\n",
    "\n",
    "print(f\"\\nExport complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Cell 11: Preview Dataset\n",
    "\n",
    "Display sample records from the final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET PREVIEW\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Display first 20 rows with formatted output\n",
    "print(\"\\nFirst 20 records (metadata only):\")\n",
    "print(\"\\n\")\n",
    "\n",
    "preview_df = combined_df[['id', 'label', 'type', 'language', 'problem_id']].head(20)\n",
    "print(preview_df.to_string(index=True))\n",
    "\n",
    "# Show sample code snippets for different clone types\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SAMPLE CODE PAIRS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for clone_type in ['type1', 'type2', 'type3', 'type4', 'negative_easy', 'negative_hard']:\n",
    "    samples = combined_df[combined_df['type'] == clone_type].head(1)\n",
    "    \n",
    "    if len(samples) == 0:\n",
    "        print(f\"\\nNo samples available for {clone_type}\")\n",
    "        continue\n",
    "    \n",
    "    sample = samples.iloc[0]\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Example: {clone_type.upper()}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"ID: {sample['id']}\")\n",
    "    print(f\"Label: {sample['label']} ({'Clone' if sample['label'] == 1 else 'Non-clone'})\")\n",
    "    print(f\"Language: {sample['language']}\")\n",
    "    print(f\"Problem: {sample['problem_id']}\")\n",
    "    \n",
    "    print(f\"\\nCode 1 (first 300 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(sample['code_1'][:300] + (\"...\" if len(sample['code_1']) > 300 else \"\"))\n",
    "    \n",
    "    print(f\"\\nCode 2 (first 300 chars):\")\n",
    "    print(\"-\" * 80)\n",
    "    print(sample['code_2'][:300] + (\"...\" if len(sample['code_2']) > 300 else \"\"))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nSuccessfully generated {len(combined_df):,} code clone pairs\")\n",
    "print(f\"\\nOutput files:\")\n",
    "print(f\"   Main dataset: {main_output_file}\")\n",
    "print(f\"   Timestamped: {output_filepath}\")\n",
    "print(f\"   Statistics: {stats_file}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
