#!/usr/bin/env python3
"""
Clone Detection Evaluation Module.

This module computes precision, recall, and F1-score for clone detection results,
providing per-clone-type metrics (T1, T2, T3, T4) and overall performance.

Metrics Computed:
- Precision: Of all detected clones of type X, what % are correct?
- Recall: Of all actual clones of type X, what % were detected?
- F1-Score: Harmonic mean of precision and recall

Evaluation Strategy:
1. Load ground truth clone pairs (from T1-T4 detection outputs)
2. Load predicted clone pairs (from model predictions or validation)
3. Compare at pair level with highest-type-wins policy
4. Compute per-type and aggregate metrics

Use Cases:
- Validating detection pipeline accuracy
- Comparing different detection strategies
- Benchmarking ML model performance
- Thesis evaluation and reporting
"""

import logging
import json
from pathlib import Path
from typing import Dict, List, Tuple, Set
from collections import defaultdict, Counter

from utils import (
    PathManager,
    MetadataLoader,
    ClonePairDeduplicator,
    save_json,
    ensure_dir
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class CloneEvaluator:
    """
    Main evaluator for clone detection results.
    
    Computes precision, recall, and F1 per clone type by comparing
    ground truth pairs against predicted pairs.
    """
    
    def __init__(self, base_dir: Path):
        """
        Initialize evaluator.
        
        Args:
            base_dir: Root directory of clone detection project
        """
        self.base_dir = Path(base_dir)
        self.paths = PathManager(base_dir)
        self.loader = MetadataLoader(self.paths)
        
        # Ground truth and predictions
        self.ground_truth = {}  # (id1, id2) -> label
        self.predictions = {}   # (id1, id2) -> label
    
    def load_ground_truth(self) -> None:
        """
        Load ground truth clone pairs from detection outputs.
        
        This loads the T1-T4 pairs generated by the detection pipeline
        as the ground truth for evaluation.
        """
        logger.info("Loading ground truth clone pairs...")
        
        t1_pairs = self.loader.load_clone_pairs('t1')
        t2_pairs = self.loader.load_clone_pairs('t2')
        t3_pairs = self.loader.load_clone_pairs('t3')
        t4_pairs = self.loader.load_clone_pairs('t4')
        
        logger.info(f"Loaded ground truth: T1={len(t1_pairs)}, T2={len(t2_pairs)}, "
                   f"T3={len(t3_pairs)}, T4={len(t4_pairs)}")
        
        # Deduplicate with highest type wins
        self.ground_truth = ClonePairDeduplicator.deduplicate(
            t1_pairs, t2_pairs, t3_pairs, t4_pairs
        )
        
        logger.info(f"Ground truth total: {len(self.ground_truth)} pairs")
        
        # Log distribution
        label_counts = Counter(self.ground_truth.values())
        for label in sorted(label_counts.keys()):
            logger.info(f"  Type-{label}: {label_counts[label]} pairs")
    
    def load_predictions(self, predictions_file: Path) -> None:
        """
        Load predicted clone pairs from file.
        
        Args:
            predictions_file: Path to predictions file (JSONL or CSV)
        
        Expected format (JSONL):
            {"id_a": "...", "id_b": "...", "predicted_label": 0-4}
        
        Expected format (CSV):
            id_a,id_b,predicted_label
        """
        logger.info(f"Loading predictions from {predictions_file}...")
        
        if not predictions_file.exists():
            raise FileNotFoundError(f"Predictions file not found: {predictions_file}")
        
        # Determine format and load
        if predictions_file.suffix == '.jsonl':
            self._load_predictions_jsonl(predictions_file)
        elif predictions_file.suffix == '.csv':
            self._load_predictions_csv(predictions_file)
        else:
            raise ValueError(f"Unsupported predictions format: {predictions_file.suffix}")
        
        logger.info(f"Loaded {len(self.predictions)} predictions")
        
        # Log distribution
        label_counts = Counter(self.predictions.values())
        for label in sorted(label_counts.keys()):
            logger.info(f"  Predicted Type-{label}: {label_counts[label]} pairs")
    
    def _load_predictions_jsonl(self, path: Path) -> None:
        """Load predictions from JSONL file."""
        import json
        
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    entry = json.loads(line)
                    id_a = entry['id_a']
                    id_b = entry['id_b']
                    label = entry['predicted_label']
                    
                    pair = ClonePairDeduplicator.normalize_pair(id_a, id_b)
                    self.predictions[pair] = label
    
    def _load_predictions_csv(self, path: Path) -> None:
        """Load predictions from CSV file."""
        import csv
        
        with open(path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            for row in reader:
                id_a = row['id_a']
                id_b = row['id_b']
                label = int(row['predicted_label'])
                
                pair = ClonePairDeduplicator.normalize_pair(id_a, id_b)
                self.predictions[pair] = label
    
    def use_ground_truth_as_predictions(self) -> None:
        """
        Use ground truth as predictions for validation.
        
        This is useful for validating the evaluation pipeline itself
        (should achieve perfect scores) or for evaluating the detection
        pipeline's internal consistency.
        """
        logger.info("Using ground truth as predictions (validation mode)")
        self.predictions = self.ground_truth.copy()
    
    def compute_metrics_per_type(self) -> Dict[int, Dict[str, float]]:
        """
        Compute precision, recall, F1 for each clone type.
        
        Returns:
            Dict mapping clone_type to {precision, recall, f1}
        """
        logger.info("Computing per-type metrics...")
        
        metrics = {}
        
        for clone_type in [1, 2, 3, 4]:
            # Get ground truth and predicted pairs for this type
            gt_pairs = {pair for pair, label in self.ground_truth.items() if label == clone_type}
            pred_pairs = {pair for pair, label in self.predictions.items() if label == clone_type}
            
            # Compute metrics
            true_positives = len(gt_pairs & pred_pairs)
            false_positives = len(pred_pairs - gt_pairs)
            false_negatives = len(gt_pairs - pred_pairs)
            
            # Precision: TP / (TP + FP)
            precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0.0
            
            # Recall: TP / (TP + FN)
            recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0.0
            
            # F1: 2 * (P * R) / (P + R)
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0.0
            
            metrics[clone_type] = {
                'precision': precision,
                'recall': recall,
                'f1': f1,
                'true_positives': true_positives,
                'false_positives': false_positives,
                'false_negatives': false_negatives,
                'ground_truth_count': len(gt_pairs),
                'predicted_count': len(pred_pairs)
            }
            
            logger.info(f"Type-{clone_type}: P={precision:.3f}, R={recall:.3f}, F1={f1:.3f}")
        
        return metrics
    
    def compute_aggregate_metrics(self) -> Dict[str, float]:
        """
        Compute aggregate metrics across all clone types.
        
        Returns:
            Dict with macro-averaged and micro-averaged metrics
        """
        logger.info("Computing aggregate metrics...")
        
        # Macro-average: average of per-type metrics
        per_type = self.compute_metrics_per_type()
        
        macro_precision = sum(m['precision'] for m in per_type.values()) / len(per_type)
        macro_recall = sum(m['recall'] for m in per_type.values()) / len(per_type)
        macro_f1 = sum(m['f1'] for m in per_type.values()) / len(per_type)
        
        # Micro-average: aggregate TP/FP/FN across all types
        total_tp = sum(m['true_positives'] for m in per_type.values())
        total_fp = sum(m['false_positives'] for m in per_type.values())
        total_fn = sum(m['false_negatives'] for m in per_type.values())
        
        micro_precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
        micro_recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0
        micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall) if (micro_precision + micro_recall) > 0 else 0.0
        
        aggregate = {
            'macro_precision': macro_precision,
            'macro_recall': macro_recall,
            'macro_f1': macro_f1,
            'micro_precision': micro_precision,
            'micro_recall': micro_recall,
            'micro_f1': micro_f1
        }
        
        logger.info(f"Macro-avg: P={macro_precision:.3f}, R={macro_recall:.3f}, F1={macro_f1:.3f}")
        logger.info(f"Micro-avg: P={micro_precision:.3f}, R={micro_recall:.3f}, F1={micro_f1:.3f}")
        
        return aggregate
    
    def generate_report(self) -> Dict:
        """
        Generate complete evaluation report.
        
        Returns:
            Dict with per-type metrics, aggregate metrics, and metadata
        """
        logger.info("Generating evaluation report...")
        
        per_type = self.compute_metrics_per_type()
        aggregate = self.compute_aggregate_metrics()
        
        report = {
            'per_type_metrics': {
                f'T{k}': v for k, v in per_type.items()
            },
            'aggregate_metrics': aggregate,
            'summary': {
                'total_ground_truth_pairs': len(self.ground_truth),
                'total_predicted_pairs': len(self.predictions),
                'ground_truth_distribution': dict(Counter(self.ground_truth.values())),
                'predicted_distribution': dict(Counter(self.predictions.values()))
            }
        }
        
        return report
    
    def save_report(self, report: Dict) -> None:
        """
        Save evaluation report to JSON file.
        
        Args:
            report: Evaluation report dict
        """
        output_path = self.paths.get_eval_metrics_path()
        ensure_dir(output_path.parent)
        
        logger.info(f"Saving report to {output_path}...")
        save_json(report, output_path)
        logger.info("Report saved successfully")
    
    def print_report(self, report: Dict) -> None:
        """
        Print formatted evaluation report.
        
        Args:
            report: Evaluation report dict
        """
        print("\n" + "="*80)
        print("CLONE DETECTION EVALUATION REPORT")
        print("="*80)
        
        print("\nPer-Type Metrics:")
        print("-"*80)
        print(f"{'Type':<8} {'Precision':<12} {'Recall':<12} {'F1':<12} {'GT':<8} {'Pred':<8}")
        print("-"*80)
        
        for type_name in ['T1', 'T2', 'T3', 'T4']:
            metrics = report['per_type_metrics'][type_name]
            print(f"{type_name:<8} "
                  f"{metrics['precision']:<12.3f} "
                  f"{metrics['recall']:<12.3f} "
                  f"{metrics['f1']:<12.3f} "
                  f"{metrics['ground_truth_count']:<8} "
                  f"{metrics['predicted_count']:<8}")
        
        print("-"*80)
        
        agg = report['aggregate_metrics']
        print(f"\n{'Aggregate Metrics:':<20}")
        print(f"  Macro-avg Precision: {agg['macro_precision']:.3f}")
        print(f"  Macro-avg Recall:    {agg['macro_recall']:.3f}")
        print(f"  Macro-avg F1:        {agg['macro_f1']:.3f}")
        print(f"  Micro-avg Precision: {agg['micro_precision']:.3f}")
        print(f"  Micro-avg Recall:    {agg['micro_recall']:.3f}")
        print(f"  Micro-avg F1:        {agg['micro_f1']:.3f}")
        
        summary = report['summary']
        print(f"\n{'Summary:':<20}")
        print(f"  Total ground truth pairs: {summary['total_ground_truth_pairs']}")
        print(f"  Total predicted pairs:    {summary['total_predicted_pairs']}")
        
        print("="*80 + "\n")
    
    def run(self, predictions_file: Path = None, validate_mode: bool = False) -> Dict:
        """
        Execute complete evaluation pipeline.
        
        Args:
            predictions_file: Path to predictions file (if None, uses ground truth)
            validate_mode: If True, use ground truth as predictions
        
        Returns:
            Evaluation report dict
        """
        logger.info("="*80)
        logger.info("CLONE DETECTION EVALUATION")
        logger.info("="*80)
        
        # Load ground truth
        self.load_ground_truth()
        
        # Load predictions
        if validate_mode:
            self.use_ground_truth_as_predictions()
        elif predictions_file:
            self.load_predictions(predictions_file)
        else:
            logger.info("No predictions provided, using ground truth for validation")
            self.use_ground_truth_as_predictions()
        
        # Generate report
        report = self.generate_report()
        
        # Print report
        self.print_report(report)
        
        # Save report
        self.save_report(report)
        
        logger.info("Evaluation completed successfully!")
        
        return report


def main():
    """Main entry point for evaluation."""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Evaluate clone detection results"
    )
    parser.add_argument(
        '--predictions',
        type=str,
        help='Path to predictions file (JSONL or CSV)'
    )
    parser.add_argument(
        '--validate',
        action='store_true',
        help='Validation mode: use ground truth as predictions (should achieve perfect scores)'
    )
    
    args = parser.parse_args()
    
    # Determine base directory
    script_path = Path(__file__).resolve()
    base_dir = script_path.parent
    
    # Initialize evaluator
    evaluator = CloneEvaluator(base_dir)
    
    # Run evaluation
    predictions_path = Path(args.predictions) if args.predictions else None
    evaluator.run(predictions_path, args.validate)


if __name__ == '__main__':
    main()
