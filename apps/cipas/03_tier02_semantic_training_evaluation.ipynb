{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15f1f5a5",
   "metadata": {},
   "source": [
    "# Tier 02: Semantic Analysis (UniXcoder) - Training & Evaluation\n",
    "\n",
    "This notebook fine-tunes **UniXcoder** for semantic clone detection using AST-flattened representations.\n",
    "\n",
    "**Tier 02 Overview:**\n",
    "- **Approach**: AST-based semantic embeddings with fine-tuned UniXcoder\n",
    "- **Model**: UniXcoder (RoBERTa-based) with InfoNCE loss\n",
    "- **Input**: AST-flattened code sequences\n",
    "- **Output**: Similarity scores for semantic clone detection\n",
    "- **Purpose**: Handles ambiguous cases from Tier 1 (0.4 < P < 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f03602",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc6e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add project root to path\n",
    "BASE_DIR = Path.cwd()\n",
    "sys.path.append(str(BASE_DIR))\n",
    "\n",
    "# Check GPU availability\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(f\"Device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f2a69a",
   "metadata": {},
   "source": [
    "## Step 2: Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a621675",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = BASE_DIR / \"datasets\" / \"processing\" / \"unixcoder_training_data.parquet\"\n",
    "\n",
    "if DATA_PATH.exists():\n",
    "    df = pd.read_parquet(DATA_PATH)\n",
    "    print(f\"✓ Loaded {len(df)} training samples\")\n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(f\"  Columns: {list(df.columns)}\")\n",
    "    if 'label' in df.columns:\n",
    "        print(f\"  Label distribution:\")\n",
    "        print(df['label'].value_counts())\n",
    "else:\n",
    "    print(f\"✗ Error: Training data not found at {DATA_PATH}\")\n",
    "    print(\"  Please run the data preparation notebook first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943d31a6",
   "metadata": {},
   "source": [
    "## Step 3: Fine-tune UniXcoder\n",
    "\n",
    "Train UniXcoder with InfoNCE loss on AST-flattened code representations.\n",
    "\n",
    "**Training Configuration:**\n",
    "- Model: microsoft/unixcoder-base\n",
    "- Loss: InfoNCE (contrastive learning)\n",
    "- Batch size: 16 (configurable)\n",
    "- Learning rate: 2e-5\n",
    "- Epochs: 3-5\n",
    "\n",
    "**Note:** This step may take significant time (hours) depending on dataset size and hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195316bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic.scripts import train_unixcoder\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FINE-TUNING UNIXCODER\")\n",
    "print(\"=\"*60)\n",
    "print(\"This may take several hours depending on your hardware...\")\n",
    "\n",
    "# Run training\n",
    "# You can modify parameters in the train_unixcoder.py script if needed\n",
    "train_unixcoder.main()\n",
    "\n",
    "# Check if model was saved\n",
    "MODEL_DIR = BASE_DIR / \"semantic\" / \"models\" / \"unixcoder_finetuned\"\n",
    "if MODEL_DIR.exists():\n",
    "    print(f\"\\n✓ Model saved to: {MODEL_DIR}\")\n",
    "    print(f\"  Model files: {list(MODEL_DIR.glob('*'))}\")\n",
    "else:\n",
    "    print(\"\\n✗ Error: Model not saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486b2f60",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate UniXcoder\n",
    "\n",
    "Evaluate the fine-tuned model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51671f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic.scripts import evaluate_unixcoder\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"EVALUATING TIER 2 (SEMANTIC)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Run evaluation\n",
    "evaluate_unixcoder.main()\n",
    "\n",
    "print(\"\\n✓ Tier 2 evaluation completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df4f19",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Tier 2 (Semantic - UniXcoder) training and evaluation completed!\n",
    "\n",
    "**What was done:**\n",
    "- ✓ Fine-tuned UniXcoder on AST-flattened code representations\n",
    "- ✓ Trained with InfoNCE contrastive loss\n",
    "- ✓ Evaluated model performance on semantic similarity\n",
    "- ✓ Saved model to `semantic/models/unixcoder_finetuned/`\n",
    "\n",
    "**How it works:**\n",
    "1. Code is flattened into AST sequences\n",
    "2. UniXcoder generates embeddings\n",
    "3. Cosine similarity determines clone probability\n",
    "4. Handles ambiguous cases from Tier 1 (0.4 < P < 0.8)\n",
    "\n",
    "Cases still ambiguous after Tier 2 can be sent to Tier 3 (Provenance Analysis)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
