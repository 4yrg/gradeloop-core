{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac8e31d1",
   "metadata": {},
   "source": [
    "# Data Preparation Pipeline\n",
    "\n",
    "This notebook prepares the dataset for training the code clone detection system by:\n",
    "1. Filtering BigCloneBench (BCB) data\n",
    "2. Extracting code fragments\n",
    "3. Normalizing fragments\n",
    "4. Creating unified dataset\n",
    "\n",
    "**Pipeline Steps:**\n",
    "- `01_filter_bcb.py`: Filters BCB dataset for clone types 1, 2, and 3\n",
    "- `02_extract_fragments.py`: Extracts method fragments using Tree-sitter\n",
    "- `03_normalize.py`: Normalizes code with blind renaming\n",
    "- `04_create_unified.py`: Creates unified training dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19435ad7",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb74277",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Add project root to path\n",
    "BASE_DIR = Path.cwd()\n",
    "sys.path.append(str(BASE_DIR))\n",
    "\n",
    "print(f\"Working directory: {BASE_DIR}\")\n",
    "print(\"✓ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28fcca5",
   "metadata": {},
   "source": [
    "## Step 2: Filter BCB Dataset\n",
    "\n",
    "Filter the BigCloneBench dataset to keep clone types 1, 2, and 3, remove missing code, and generate method IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a73eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the script as a module\n",
    "from datasets.scripts import filter_bcb as step1\n",
    "\n",
    "# Run the filtering\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 1: FILTERING BCB DATASET\")\n",
    "print(\"=\"*60)\n",
    "step1.main()\n",
    "\n",
    "# Verify output\n",
    "csv_path = BASE_DIR / \"datasets\" / \"processing\" / \"bcb_filtered.csv\"\n",
    "if csv_path.exists():\n",
    "    df = pd.read_csv(csv_path)\n",
    "    print(f\"\\n✓ Filtered dataset created: {len(df)} pairs\")\n",
    "    print(f\"  Clone type distribution:\")\n",
    "    print(df['clone_type'].value_counts())\n",
    "else:\n",
    "    print(\"✗ Error: Filtered dataset not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d77b0",
   "metadata": {},
   "source": [
    "## Step 3: Extract Code Fragments\n",
    "\n",
    "Parse Java files and extract method-level code fragments using Tree-sitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702ea99",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.scripts import extract_fragments as step2\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 2: EXTRACTING CODE FRAGMENTS\")\n",
    "print(\"=\"*60)\n",
    "step2.main()\n",
    "\n",
    "# Verify output\n",
    "jsonl_path = BASE_DIR / \"datasets\" / \"processing\" / \"fragments.jsonl\"\n",
    "if jsonl_path.exists():\n",
    "    with open(jsonl_path, 'r') as f:\n",
    "        fragments = [json.loads(line) for line in f]\n",
    "    print(f\"\\n✓ Extracted {len(fragments)} fragments\")\n",
    "    print(f\"  Sample fragment keys: {list(fragments[0].keys()) if fragments else 'None'}\")\n",
    "else:\n",
    "    print(\"✗ Error: Fragments file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e4177",
   "metadata": {},
   "source": [
    "## Step 4: Normalize Code Fragments\n",
    "\n",
    "Apply blind renaming to normalize identifiers and literals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4993dae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.scripts import normalize as step3\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 3: NORMALIZING FRAGMENTS\")\n",
    "print(\"=\"*60)\n",
    "step3.main()\n",
    "\n",
    "# Verify output\n",
    "norm_path = BASE_DIR / \"datasets\" / \"processing\" / \"fragments_normalized.jsonl\"\n",
    "if norm_path.exists():\n",
    "    with open(norm_path, 'r') as f:\n",
    "        normalized = [json.loads(line) for line in f]\n",
    "    print(f\"\\n✓ Normalized {len(normalized)} fragments\")\n",
    "    if normalized:\n",
    "        print(f\"  Sample normalized code: {normalized[0].get('normalized_code', '')[:100]}...\")\n",
    "else:\n",
    "    print(\"✗ Error: Normalized fragments file not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5c4b45",
   "metadata": {},
   "source": [
    "## Step 5: Create Unified Training Dataset\n",
    "\n",
    "Create a unified dataset combining BCB and CodeNet data with train/validation/test splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f7f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.scripts import create_unified as step4\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"STEP 4: CREATING UNIFIED DATASET\")\n",
    "print(\"=\"*60)\n",
    "step4.main()\n",
    "\n",
    "# Verify output\n",
    "unified_dir = BASE_DIR / \"datasets\" / \"processing\" / \"unified\"\n",
    "if unified_dir.exists():\n",
    "    files = list(unified_dir.glob(\"*.parquet\"))\n",
    "    print(f\"\\n✓ Created unified dataset with {len(files)} files:\")\n",
    "    for f in sorted(files):\n",
    "        df_temp = pd.read_parquet(f)\n",
    "        print(f\"  - {f.name}: {len(df_temp)} rows\")\n",
    "else:\n",
    "    print(\"✗ Error: Unified dataset directory not created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff50e99a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Data preparation pipeline completed! The following files have been created:\n",
    "\n",
    "1. **bcb_filtered.csv**: Filtered clone pairs\n",
    "2. **fragments.jsonl**: Extracted code fragments\n",
    "3. **fragments_normalized.jsonl**: Normalized fragments\n",
    "4. **unified/*.parquet**: Training datasets for all tiers\n",
    "\n",
    "You can now proceed to training notebooks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
