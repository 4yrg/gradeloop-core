{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data Processing & Normalization Evaluation\n",
                "\n",
                "This notebook evaluates the performance and quality of the text normalization and tokenization steps, which are critical for Tier 1 detection."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import pandas as pd\n",
                "import time\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# Add parent directory to path to import app modules\n",
                "sys.path.append(os.path.abspath(\"../..\"))\n",
                "\n",
                "from apps.cipas.app.features.normalization import normalize_code_ast, tokenize_source"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Load Dataset\n",
                "Loading a sample from BigCloneBench or using mock data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try loading local parquet if exists, else mock\n",
                "parquet_path = \"../datasets/raw/bigclonebench_400k.parquet\"\n",
                "try:\n",
                "    df = pd.read_parquet(parquet_path).head(1000)\n",
                "    print(f\"Loaded {len(df)} samples from {parquet_path}\")\n",
                "    # Assuming column 'code' or 'func_code' exists. Adjust as needed.\n",
                "    if 'code' not in df.columns and 'func_code' in df.columns:\n",
                "        df['code'] = df['func_code']\n",
                "except FileNotFoundError:\n",
                "    print(\"Dataset not found. Using mock data.\")\n",
                "    data = [\n",
                "        {\"id\": \"1\", \"code\": \"public void test() { int a = 10; System.out.println(a); }\"},\n",
                "        {\"id\": \"2\", \"code\": \"public void test2() { int b = 20; System.out.println(b); }\"},\n",
                "        {\"id\": \"3\", \"code\": \"// Comment\\npublic int add(int x, int y) { return x + y; }\"}\n",
                "    ]\n",
                "    df = pd.DataFrame(data)\n",
                "\n",
                "print(df.head())"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Normalization Performance\n",
                "Measuring the time taken to normalize code and the reduction in vocabulary size."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "start_time = time.time()\n",
                "\n",
                "df['normalized_code'] = df['code'].apply(normalize_code_ast)\n",
                "\n",
                "end_time = time.time()\n",
                "print(f\"Normalization Time for {len(df)} items: {end_time - start_time:.4f} seconds\")\n",
                "print(f\"Average per item: {(end_time - start_time)/len(df):.4f} seconds\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Example Normalization"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Original:\")\n",
                "print(df.iloc[0]['code'])\n",
                "print(\"\\nNormalized:\")\n",
                "print(df.iloc[0]['normalized_code'])"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Tokenization Statistics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "df['tokens'] = df['normalized_code'].apply(tokenize_source)\n",
                "df['num_tokens'] = df['tokens'].apply(len)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.hist(df['num_tokens'], bins=30, alpha=0.7)\n",
                "plt.title('Distribution of Token Counts after Normalization')\n",
                "plt.xlabel('Number of Tokens')\n",
                "plt.ylabel('Frequency')\n",
                "plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}