{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Tier 1: Lexical Detection Evaluation\n",
                "\n",
                "Evaluates the hash-based (Type-1) and token-overlap (Type-2) detection tiers.\n",
                "**Pre-requisite**: Redis must be running locally on port 6379."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Redis connection successful\n"
                    ]
                }
            ],
            "source": [
                "import sys\n",
                "import os\n",
                "import asyncio\n",
                "import pandas as pd\n",
                "from typing import List\n",
                "\n",
                "sys.path.append(os.path.abspath(\"..\"))\n",
                "os.environ[\"REDIS_URL\"] = \"redis://localhost:6379/0\"\n",
                "\n",
                "from app.pipeline.tier1_lexical import tier1\n",
                "from app.features.normalization import normalize_code_ast, tokenize_source\n",
                "from app.models.submission import Submission\n",
                "\n",
                "# Ensure we can talk to Redis\n",
                "try:\n",
                "    import redis\n",
                "    r = redis.Redis.from_url(\"redis://localhost:6379/0\")\n",
                "    r.ping()\n",
                "    print(\"Redis connection successful\")\n",
                "    r.flushdb() # Clean start for eval\n",
                "except Exception as e:\n",
                "    print(f\"Warning: Redis not connected. {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Prepare Test Data\n",
                "Creating a set of synthetic Type-1 and Type-2 clones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Test Cases Prepared\n"
                    ]
                }
            ],
            "source": [
                "test_cases = [\n",
                "    (\"s1\", \"public void foo() { int x = 1; }\", \"Original\"),\n",
                "    (\"s2\", \"public void foo() { int x = 1; }\", \"Type-1 (Exact)\"),\n",
                "    (\"s3\", \"public void foo() { int y = 1; }\", \"Type-2 (Rename x->y)\"),\n",
                "    (\"s4\", \"public void bar() { float z = 2.0; }\", \"Different\")\n",
                "]\n",
                "\n",
                "submissions = []\n",
                "for tid, code, label in test_cases:\n",
                "    submissions.append(Submission(id=tid, student_id=\"student\", assignment_id=\"a1\", code=code))\n",
                "\n",
                "print(\"Test Cases Prepared\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Indexing Performance\n",
                "Measure time to index submissions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Indexing Time: 0.0215s\n"
                    ]
                }
            ],
            "source": [
                "import time\n",
                "\n",
                "start = time.time()\n",
                "\n",
                "async def index_all():\n",
                "    for sub in submissions:\n",
                "        norm = normalize_code_ast(sub.code)\n",
                "        tokens = tokenize_source(norm)\n",
                "        await tier1.index_submission(sub, tokens)\n",
                "\n",
                "await index_all()\n",
                "end = time.time()\n",
                "\n",
                "print(f\"Indexing Time: {end - start:.4f}s\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Query & Precision Evaluation\n",
                "Query with a Type-2 clone and check results."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Query Results:\n",
                        "{'submission_id': 's1', 'similarity': 1.0, 'tier': 1, 'type': 'Type-1'}\n",
                        "{'submission_id': 's2', 'similarity': 1.0, 'tier': 1, 'type': 'Type-1'}\n",
                        "{'submission_id': 's3', 'similarity': 1.0, 'tier': 1, 'type': 'Type-1'}\n",
                        "{'submission_id': 's4', 'similarity': 0.8888888888888888, 'tier': 1, 'type': 'Candidate'}\n",
                        "Recall: 1.00\n"
                    ]
                }
            ],
            "source": [
                "query_code = \"public void foo() { int z = 1; }\" # Type-2 of s1\n",
                "\n",
                "norm = normalize_code_ast(query_code)\n",
                "tokens = tokenize_source(norm)\n",
                "\n",
                "results = await tier1.search(tokens)\n",
                "print(\"Query Results:\")\n",
                "for r in results:\n",
                "    print(r)\n",
                "\n",
                "# Evaluation Metric: Recall\n",
                "# Expecting s1, s2, s3 to be in results (since they normalize similarly)\n",
                "expected_ids = {'s1', 's2', 's3'}\n",
                "retrieved_ids = {r['submission_id'] for r in results}\n",
                "\n",
                "recall = len(expected_ids.intersection(retrieved_ids)) / len(expected_ids)\n",
                "print(f\"Recall: {recall:.2f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "8bc39c97",
            "metadata": {},
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": ".venv",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.3"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
