{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Full Pipeline Evaluation\n",
                "\n",
                "Evaluates the end-to-end detection capability using the Orchestrator.\n",
                "**Metrics**: Total Latency, Throughput, Detection Accuracy by Type."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import os\n",
                "import time\n",
                "import asyncio\n",
                "import pandas as pd\n",
                "\n",
                "sys.path.append(os.path.abspath(\"../..\"))\n",
                "os.environ[\"REDIS_URL\"] = \"redis://localhost:6379/0\"\n",
                "\n",
                "from apps.cipas.app.pipeline.orchestrator import orchestrator\n",
                "from apps.cipas.app.models.submission import Submission"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Data\n",
                "Create a mix of Type 1-4 clones."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "base_code = \"public int add(int a, int b) { return a + b; }\"\n",
                "type1 = \"public int add(int a, int b) { return a + b; }\"\n",
                "type2 = \"public int sum(int x, int y) { return x + y; }\"\n",
                "type3 = \"public int add(int a, int b) { int res = a + b; return res; }\"\n",
                "type4 = \"public int add(int a, int b) { if(b==0) return a; return add(a+1, b-1); }\" # recursive, dif algo but checks\n",
                "\n",
                "dataset = [\n",
                "    (\"sub_base\", base_code),\n",
                "    (\"sub_t1\", type1),\n",
                "    (\"sub_t2\", type2),\n",
                "    (\"sub_t3\", type3),\n",
                "    (\"sub_t4\", type4),\n",
                "]\n",
                "\n",
                "# Pre-index the 'base' submission so others can find it\n",
                "base_sub = Submission(id=\"sub_base\", student_id=\"s0\", assignment_id=\"a1\", code=base_code)\n",
                "await orchestrator.processed_submission(base_sub)\n",
                "print(\"Base submission indexed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Run Pipeline & Measure Latency"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "results = []\n",
                "detailed_logs = []\n",
                "\n",
                "for sid, code in dataset[1:]: # Skip base\n",
                "    start_t = time.time()\n",
                "    \n",
                "    sub = Submission(id=sid, student_id=sid, assignment_id=\"a1\", code=code)\n",
                "    res = await orchestrator.processed_submission(sub)\n",
                "    \n",
                "    end_t = time.time()\n",
                "    latency = end_t - start_t\n",
                "    \n",
                "    # Get top match regarding base\n",
                "    top_match = next((m for m in res['matches'] if m['submission_id'] == 'sub_base'), None)\n",
                "    \n",
                "    results.append({\n",
                "        \"submission_id\": sid,\n",
                "        \"latency_ms\": latency * 1000,\n",
                "        \"found_match\": top_match is not None,\n",
                "        \"clone_type\": top_match['clone_type'] if top_match else \"None\",\n",
                "        \"score\": top_match['final_score'] if top_match else 0.0\n",
                "    })\n",
                "\n",
                "df_res = pd.DataFrame(results)\n",
                "print(df_res)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Evaluation Metrics"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "avg_latency = df_res['latency_ms'].mean()\n",
                "max_latency = df_res['latency_ms'].max()\n",
                "\n",
                "print(f\"Average Latency: {avg_latency:.2f} ms\")\n",
                "print(f\"Max Latency: {max_latency:.2f} ms\")\n",
                "\n",
                "# Check correctness (Expectations)\n",
                "# sub_t1 -> Type-1/2\n",
                "# sub_t2 -> Type-1/2 or Type-2\n",
                "# sub_t3 -> Type-3\n",
                "# sub_t4 -> Type-4\n",
                "\n",
                "print(\"\\nPerformance Summary:\")\n",
                "print(df_res[['submission_id', 'clone_type', 'score']])"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}