"""
Type-3 clone generation using back-translation approach.

Type-3 clones have similar functionality but with statement-level modifications
such as adding/removing statements, changing control flow, or restructuring code.
This module uses a back-translation approach: code → summary → regenerated code.

Type-3 Transformations (must remain mostly similar):
    - Identifier renaming (same as Type-2)
    - Literal changes
    - Formatting and comments
    - Data type modifications
    - Adding or removing statements (extra lines, missing lines, added checks)
    - Small helper operations
    - Reordering closely related statements (preserving control flow)
    - Small changes in control flow (extra if guards, logging, added computation steps)

Critical Requirements:
    - The two code fragments must be MOSTLY SIMILAR
    - Core logic remains RECOGNIZABLE and ALIGNED
    - NOT completely rewritten (otherwise becomes Type-4)

The LLM-based approach:
1. Summarize original code to capture high-level functionality
2. Generate new code from summary with Type-3 modifications
3. Result is functionally similar but structurally different within Type-3 constraints

This module provides a mock implementation with a clean adapter interface
that can be replaced with real LLM backends (OpenAI, Anthropic, local models).

Classes:
    BaseLLMClient: Abstract base class for LLM adapters
    MockLLMClient: Mock implementation for testing and development
    
Functions:
    produce_type3: Generate Type-3 clone using back-translation
    create_llm_client: Factory function for LLM client creation
    
See Also:
    - TYPE3_CLONE_SPECIFICATION.md: Complete Type-3 specification
    - ollama_client.py: Production LLM client for Type-3 generation
"""

import logging
import re
from abc import ABC, abstractmethod
from typing import Any

logger = logging.getLogger(__name__)


class BaseLLMClient(ABC):
    """
    Abstract base class for LLM client adapters.
    
    This interface defines the contract that all LLM backends must implement.
    Allows swapping between mock, OpenAI, Anthropic, or local models without
    changing the clone generation code.
    """
    
    @abstractmethod
    def summarize(self, code: str, lang: str) -> str:
        """
        Generate a high-level summary of code functionality.
        
        Args:
            code: Source code to summarize
            lang: Programming language
            
        Returns:
            Natural language summary of what the code does
        """
        pass
    
    @abstractmethod
    def generate_from_summary(self, summary: str, lang: str) -> str:
        """
        Generate new code from a functional summary.
        
        Args:
            summary: Natural language description of desired functionality
            lang: Target programming language
            
        Returns:
            Generated source code implementing the summary
        """
        pass


class MockLLMClient(BaseLLMClient):
    """
    Mock LLM client for testing and development.
    
    This implementation simulates LLM behavior without making actual API calls:
    - summarize(): Extracts a simple heuristic summary from code
    - generate_from_summary(): Returns modified original code with header comment
    
    The mock is deterministic and useful for:
    - Unit testing without API dependencies
    - Development without API costs
    - Demonstrating the adapter pattern
    
    For production use, replace with OpenAILLMClient, AnthropicLLMClient, etc.
    
    Examples:
        >>> client = MockLLMClient()
        >>> code = "def add(a, b):\\n    return a + b"
        >>> summary = client.summarize(code, "python")
        >>> "function" in summary.lower()
        True
        >>> generated = client.generate_from_summary(summary, "python")
        >>> "# generated by mock" in generated
        True
    """
    
    def __init__(self, config: dict[str, Any] | None = None):
        """
        Initialize mock LLM client.
        
        Args:
            config: Optional configuration dictionary (unused in mock)
        """
        self.config = config or {}
        logger.info("Initialized MockLLMClient")
    
    def summarize(self, code: str, lang: str) -> str:
        """
        Generate a heuristic summary from code structure.
        
        **Mock implementation**: Extracts function/method names and constructs
        a basic summary. Does not use actual LLM.
        
        Args:
            code: Source code to summarize
            lang: Programming language
            
        Returns:
            Simple heuristic summary
        """
        lang_lower = lang.lower()
        
        # Extract function/method names
        if lang_lower == "python":
            func_pattern = r'def\s+(\w+)\s*\('
        elif lang_lower == "java":
            func_pattern = r'(?:public|private|protected)?\s*\w+\s+(\w+)\s*\('
        else:
            func_pattern = r'\w+\s*\('
        
        functions = re.findall(func_pattern, code)
        
        # Count lines
        lines = [line for line in code.split('\n') if line.strip()]
        num_lines = len(lines)
        
        # Generate summary
        if functions:
            func_list = ", ".join(functions[:3])  # First 3 functions
            if len(functions) > 3:
                func_list += f", and {len(functions) - 3} more"
            summary = f"This code defines functions: {func_list}. "
        else:
            summary = "This code performs operations. "
        
        summary += f"It contains {num_lines} lines of {lang} code."
        
        # Add simple logic detection
        if "if" in code.lower():
            summary += " Includes conditional logic."
        if "for" in code.lower() or "while" in code.lower():
            summary += " Contains loops."
        
        logger.debug(f"Generated mock summary: {summary[:50]}...")
        return summary
    
    def generate_from_summary(self, summary: str, lang: str) -> str:
        """
        Generate code from summary (mock implementation).
        
        **Mock implementation**: Returns a placeholder with the summary as a
        comment. In production, this would call an LLM to generate actual code.
        
        For testing purposes, returns a simple template that includes the
        summary and indicates it's mock-generated.
        
        Args:
            summary: Functional description
            lang: Target programming language
            
        Returns:
            Mock-generated code with header comment
        """
        lang_lower = lang.lower()
        
        # Create header comment based on language
        if lang_lower == "python":
            header = f"# Generated by mock LLM client\n# Summary: {summary}\n\n"
            placeholder = "def generated_function():\n    pass\n"
        elif lang_lower == "java":
            header = f"// Generated by mock LLM client\n// Summary: {summary}\n\n"
            placeholder = "public void generatedMethod() {\n    // TODO: Implement\n}\n"
        else:
            header = f"/* Generated by mock LLM client\n   Summary: {summary} */\n\n"
            placeholder = "void generatedFunction() { }\n"
        
        generated = header + placeholder
        logger.debug(f"Generated mock code for {lang}")
        return generated


def produce_type3(
    code: str,
    lang: str,
    llm_client: BaseLLMClient
) -> str:
    """
    Generate a Type-3 clone using back-translation approach.
    
    Type-3 clones implement similar functionality with statement-level modifications:
    - Identifier renaming (variables, functions, parameters)
    - Literal value changes
    - Formatting and comment variations
    - Data type modifications
    - Adding statements (validation, logging, extra checks, helper operations)
    - Removing statements (non-essential lines)
    - Reordering closely related statements (safe reordering only)
    - Small control flow changes (extra if guards, additional logging, computation steps)
    
    Critical constraints:
    - Code fragments must be MOSTLY SIMILAR
    - Core logic remains RECOGNIZABLE and ALIGNED
    - NOT completely rewritten (that would be Type-4)
    
    Process:
    1. Summarize original code (code → summary)
    2. Generate new implementation from summary with Type-3 transformations (summary → code')
    3. Result has similar functionality but modified structure within Type-3 bounds
    
    Args:
        code: Original source code
        lang: Programming language
        llm_client: LLM adapter for summarization and generation
        
    Returns:
        Type-3 clone with similar functionality but Type-3 modifications
        
    Examples:
        >>> client = MockLLMClient()
        >>> original = "def add(a, b):\\n    return a + b"
        >>> clone = produce_type3(original, "python", client)
        >>> "generated by mock" in clone.lower()
        True
        >>> clone != original
        True
        
    See Also:
        - TYPE3_CLONE_SPECIFICATION.md for complete transformation rules
        - ollama_client.py for production LLM implementation
    """
    if not code or not code.strip():
        logger.warning("Empty or whitespace-only code provided")
        return ""
    
    try:
        # Step 1: Summarize the original code
        logger.debug(f"Summarizing {lang} code...")
        summary = llm_client.summarize(code, lang)
        
        if not summary or not summary.strip():
            logger.warning("Failed to generate summary, returning empty string")
            return ""
        
        logger.debug(f"Generated summary: {summary[:100]}...")
        
        # Step 2: Generate new code from summary
        logger.debug(f"Generating new {lang} code from summary...")
        generated_code = llm_client.generate_from_summary(summary, lang)
        
        if not generated_code or not generated_code.strip():
            logger.warning("Failed to generate code from summary, returning empty string")
            return ""
        
        logger.debug(f"Generated Type-3 clone ({len(generated_code)} chars)")
        return generated_code
        
    except Exception as e:
        logger.warning(f"Error during Type-3 generation: {e}, returning empty string to skip this sample")
        # Return empty string to indicate failure - the caller should skip this sample
        return ""


def produce_type3_with_context(
    code: str,
    lang: str,
    llm_client: BaseLLMClient,
    context: str | None = None
) -> str:
    """
    Generate Type-3 clone with additional context.
    
    Enhanced version that accepts optional context (e.g., surrounding code,
    documentation) to improve generation quality.
    
    Args:
        code: Original source code
        lang: Programming language
        llm_client: LLM adapter
        context: Optional additional context for generation
        
    Returns:
        Type-3 clone
    """
    # For now, context is logged but not used by mock
    if context:
        logger.debug(f"Using context: {context[:50]}...")
    
    return produce_type3(code, lang, llm_client)


def create_llm_client(
    provider: str = "mock",
    config: dict[str, Any] | None = None
) -> BaseLLMClient:
    """
    Factory function to create LLM client adapters.
    
    Supports multiple backends through a unified interface. Add new providers
    by implementing BaseLLMClient and registering here.
    
    Args:
        provider: LLM provider name ("mock", "openai", "anthropic", etc.)
        config: Provider-specific configuration
        
    Returns:
        LLM client instance
        
    Raises:
        ValueError: If provider is not supported
        
    Examples:
        >>> client = create_llm_client("mock")
        >>> isinstance(client, MockLLMClient)
        True
        
        >>> # For real LLM (requires implementation):
        >>> # client = create_llm_client("openai", {"api_key": "sk-..."})
    """
    provider_lower = provider.lower()
    
    if provider_lower == "mock":
        return MockLLMClient(config)
    elif provider_lower == "openai":
        # Placeholder for OpenAI implementation
        raise NotImplementedError(
            "OpenAI client not yet implemented. "
            "Implement OpenAILLMClient extending BaseLLMClient."
        )
    elif provider_lower == "anthropic":
        # Placeholder for Anthropic implementation
        raise NotImplementedError(
            "Anthropic client not yet implemented. "
            "Implement AnthropicLLMClient extending BaseLLMClient."
        )
    elif provider_lower == "local":
        # Placeholder for local model implementation
        raise NotImplementedError(
            "Local model client not yet implemented. "
            "Implement LocalLLMClient extending BaseLLMClient."
        )
    elif provider_lower == "ollama":
        from .ollama_client import OllamaLLMClient
        return OllamaLLMClient(config)
    else:
        raise ValueError(
            f"Unsupported LLM provider: {provider}. "
            f"Supported: mock, openai, anthropic, local, ollama"
        )


# Unit tests
def test_mock_llm_client_summarize():
    """Test mock client summarization."""
    client = MockLLMClient()
    code = "def add(a, b):\n    return a + b"
    summary = client.summarize(code, "python")
    
    assert isinstance(summary, str)
    assert len(summary) > 0
    assert "add" in summary
    
    print("✓ Mock summarize test passed")


def test_mock_llm_client_generate():
    """Test mock client code generation."""
    client = MockLLMClient()
    summary = "Add two numbers and return the result"
    generated = client.generate_from_summary(summary, "python")
    
    assert isinstance(generated, str)
    assert "# generated by mock" in generated.lower()
    
    print("✓ Mock generate test passed")


def test_produce_type3_python():
    """Test Type-3 generation for Python."""
    client = MockLLMClient()
    original = "def multiply(x, y):\n    return x * y"
    clone = produce_type3(original, "python", client)
    
    assert isinstance(clone, str)
    assert len(clone) > 0
    assert "generated by mock" in clone.lower()
    
    print("✓ Type-3 Python test passed")


def test_produce_type3_java():
    """Test Type-3 generation for Java."""
    client = MockLLMClient()
    original = "public int add(int a, int b) { return a + b; }"
    clone = produce_type3(original, "java", client)
    
    assert isinstance(clone, str)
    assert len(clone) > 0
    assert "generated by mock" in clone.lower()
    
    print("✓ Type-3 Java test passed")


def test_create_llm_client_mock():
    """Test LLM client factory with mock provider."""
    client = create_llm_client("mock")
    assert isinstance(client, MockLLMClient)
    
    print("✓ Client factory test passed")


def test_create_llm_client_unsupported():
    """Test error handling for unsupported provider."""
    try:
        create_llm_client("unsupported_provider")
        assert False, "Should raise ValueError"
    except ValueError as e:
        assert "Unsupported LLM provider" in str(e)
    
    print("✓ Unsupported provider test passed")


def test_empty_code():
    """Test handling of empty code."""
    client = MockLLMClient()
    assert produce_type3("", "python", client) == ""
    assert produce_type3("   ", "python", client) == "   "
    
    print("✓ Empty code test passed")


def test_base_client_interface():
    """Test that BaseLLMClient enforces interface."""
    # Cannot instantiate abstract class
    try:
        client = BaseLLMClient()
        assert False, "Should not be able to instantiate abstract class"
    except TypeError:
        pass
    
    print("✓ Base client interface test passed")


if __name__ == "__main__":
    # Run tests
    print("Running Type-3 generation tests...\n")
    
    test_mock_llm_client_summarize()
    test_mock_llm_client_generate()
    test_produce_type3_python()
    test_produce_type3_java()
    test_create_llm_client_mock()
    test_create_llm_client_unsupported()
    test_empty_code()
    test_base_client_interface()
    
    print("\nAll tests passed!")
    
    # Example usage
    print("\n--- Example Usage ---")
    
    # Create mock client
    client = create_llm_client("mock")
    
    python_example = """def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)"""
    
    print("Original Python:")
    print(python_example)
    print("\nType-3 Clone (mock):")
    print(produce_type3(python_example, "python", client))
    
    java_example = """public int factorial(int n) {
    if (n <= 1) return 1;
    return n * factorial(n - 1);
}"""
    
    print("\n\nOriginal Java:")
    print(java_example)
    print("\nType-3 Clone (mock):")
    print(produce_type3(java_example, "java", client))
    
    print("\n--- Adapter Pattern ---")
    print("To use a real LLM, implement BaseLLMClient:")
    print("""
class OpenAILLMClient(BaseLLMClient):
    def __init__(self, config):
        self.api_key = config['api_key']
        self.model = config.get('model', 'gpt-4')
    
    def summarize(self, code, lang):
        # Call OpenAI API to summarize code
        response = openai.ChatCompletion.create(...)
        return response.choices[0].message.content
    
    def generate_from_summary(self, summary, lang):
        # Call OpenAI API to generate code
        response = openai.ChatCompletion.create(...)
        return response.choices[0].message.content

# Then use it:
client = create_llm_client("openai", {"api_key": "sk-..."})
clone = produce_type3(code, "python", client)
""")
